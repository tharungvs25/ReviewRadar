{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHw0koUi7ery",
        "outputId": "84e262a9-ac6a-4974-ad26-fbfa4821f9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/129.1 kB\u001b[0m \u001b[31m649.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/129.1 kB\u001b[0m \u001b[31m764.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=192dc226c375923802b4cdc179f1974340fc758c966863d80e1a74daa01ad984\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szh6nr8JnDy2",
        "outputId": "3333972f-ba4f-4f6a-ff14-bd592dce8ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import datetime\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, Bidirectional, concatenate\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('amazon_fakereviewsDS.csv', on_bad_lines='skip')\n",
        "\n",
        "# Use only 'text_' and 'label' columns\n",
        "df = df[['text_', 'label']]\n",
        "\n",
        "# Changing the categorical values from \"CG\" and \"or\" to 0 and 1\n",
        "df['label'] = pd.Categorical(df['label'])\n",
        "df['label'] = df['label'].cat.codes\n",
        "\n",
        "def remove_punctuation(txt):\n",
        "    # We lower-case every word\n",
        "    text_lower = \"\".join([c.lower() for c in txt])\n",
        "    # Then we remove punctuations\n",
        "    txt_nonpunct = \"\".join([c for c in text_lower if c not in string.punctuation])\n",
        "    return txt_nonpunct\n",
        "\n",
        "# Apply remove_punctuation function to 'text_' column\n",
        "df['text_'] = df['text_'].apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "# Split the data in 80:10:10 for train:valid:test dataset\n",
        "train_size = 0.8\n",
        "valid_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "train_data, temp_data = train_test_split(df, train_size=train_size)\n",
        "valid_data, test_data = train_test_split(temp_data, train_size=valid_size/(valid_size + test_size))\n",
        "\n",
        "# Separate the \"text_\" and \"label\" columns\n",
        "X_train = train_data['text_']\n",
        "y_train = train_data['label']\n",
        "X_valid = valid_data['text_']\n",
        "y_valid = valid_data['label']\n",
        "X_test = test_data['text_']\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Tokenization and padding\n",
        "max_length = 200\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "X_valid_sequences = tokenizer.texts_to_sequences(X_valid)\n",
        "X_valid_padded = pad_sequences(X_valid_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Word2Vec embeddings (if needed)\n",
        "# Set up the hyperparameters for our Word2Vec embedding\n",
        "W2V_SIZE = 100\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 64\n",
        "W2V_MIN_COUNT = 5\n",
        "W2V_SG = 1\n",
        "W2V_SEED = 42\n",
        "W2V_WORKERS = 4\n",
        "\n",
        "documents = [text.split() for text in X_train]\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE,\n",
        "                                            sg=W2V_SG,\n",
        "                                            window=W2V_WINDOW,\n",
        "                                            seed=W2V_SEED,\n",
        "                                            workers=W2V_WORKERS)\n",
        "w2v_model.build_vocab(documents)\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\n",
        "\n",
        "# Embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Define the model architecture\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=W2V_SIZE,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)\n",
        "\n",
        "input_text = Input(shape=(max_length,), name='text_input')\n",
        "embeddings = embedding_layer(input_text)\n",
        "lstm = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(embeddings)\n",
        "dense = Dense(64, activation='relu')(lstm)\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "model = Model(inputs=input_text, outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_valid_padded, y_valid))\n",
        "\n",
        "# Plotting the training and validation metrics\n",
        "\"\"\"plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.show()\"\"\"\n"
      ],
      "metadata": {
        "id": "LiqN7SaRxe1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "050d60ba-ddbf-4b45-c9e7-7b25ae856a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_input (InputLayer)     [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 100)          4538300   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               84480     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4631101 (17.67 MB)\n",
            "Trainable params: 92801 (362.50 KB)\n",
            "Non-trainable params: 4538300 (17.31 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1011/1011 [==============================] - 534s 522ms/step - loss: 0.2956 - accuracy: 0.8717 - val_loss: 0.2939 - val_accuracy: 0.8875\n",
            "Epoch 2/10\n",
            "1011/1011 [==============================] - 524s 518ms/step - loss: 0.2009 - accuracy: 0.9184 - val_loss: 0.1830 - val_accuracy: 0.9251\n",
            "Epoch 3/10\n",
            "1011/1011 [==============================] - 524s 518ms/step - loss: 0.1690 - accuracy: 0.9318 - val_loss: 0.1606 - val_accuracy: 0.9335\n",
            "Epoch 4/10\n",
            "1011/1011 [==============================] - 524s 519ms/step - loss: 0.1499 - accuracy: 0.9388 - val_loss: 0.1375 - val_accuracy: 0.9463\n",
            "Epoch 5/10\n",
            "1011/1011 [==============================] - 523s 518ms/step - loss: 0.1374 - accuracy: 0.9460 - val_loss: 0.1522 - val_accuracy: 0.9384\n",
            "Epoch 6/10\n",
            "1011/1011 [==============================] - 521s 515ms/step - loss: 0.1233 - accuracy: 0.9503 - val_loss: 0.1282 - val_accuracy: 0.9493\n",
            "Epoch 7/10\n",
            "1011/1011 [==============================] - 524s 518ms/step - loss: 0.1152 - accuracy: 0.9538 - val_loss: 0.1285 - val_accuracy: 0.9555\n",
            "Epoch 8/10\n",
            "1011/1011 [==============================] - 523s 518ms/step - loss: 0.1039 - accuracy: 0.9584 - val_loss: 0.1107 - val_accuracy: 0.9587\n",
            "Epoch 9/10\n",
            "1011/1011 [==============================] - 526s 520ms/step - loss: 0.0960 - accuracy: 0.9615 - val_loss: 0.1258 - val_accuracy: 0.9515\n",
            "Epoch 10/10\n",
            "1011/1011 [==============================] - 526s 520ms/step - loss: 0.0899 - accuracy: 0.9642 - val_loss: 0.1174 - val_accuracy: 0.9542\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"plt.figure(figsize=(12, 5))\\nplt.subplot(1, 2, 1)\\nplot_graphs(history, 'accuracy')\\nplt.subplot(1, 2, 2)\\nplot_graphs(history, 'loss')\\nplt.show()\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "train_loss, train_accuracy = model.evaluate(X_train_padded, y_train)\n",
        "print(f'Training Accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Predict the classes for the test data\n",
        "y_pred = model.predict(X_test_padded)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print the classification report\n",
        "report = classification_report(y_test, y_pred_classes, target_names=['-1(fake)', '+1(real)'])\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-fQ1n4RBqPY",
        "outputId": "0b9f9819-9aef-466f-a56f-321d83f594b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1011/1011 [==============================] - 82s 82ms/step - loss: 0.0581 - accuracy: 0.9780\n",
            "Training Accuracy: 97.80%\n",
            "127/127 [==============================] - 11s 84ms/step - loss: 0.1107 - accuracy: 0.9632\n",
            "Test Accuracy: 96.32%\n",
            "127/127 [==============================] - 10s 80ms/step\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    -1(fake)       0.96      0.97      0.96      2013\n",
            "    +1(real)       0.97      0.96      0.96      2031\n",
            "\n",
            "    accuracy                           0.96      4044\n",
            "   macro avg       0.96      0.96      0.96      4044\n",
            "weighted avg       0.96      0.96      0.96      4044\n",
            "\n"
          ]
        }
      ]
    }
  ]
}